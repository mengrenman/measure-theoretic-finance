{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Conditional Expectation\n",
    "\n",
    "**Goal:** Understand conditional expectation as an $L^2$ projection, verify the tower property,\n",
    "and visualize $E[X \\mid Y]$ as a smoothing operation on the joint distribution.\n",
    "\n",
    "**Key Idea:** Given random variables $X$ and $Y$, the conditional expectation $E[X \\mid Y]$ is\n",
    "the $\\sigma(Y)$-measurable function $g(Y)$ that minimizes the mean squared error\n",
    "$E[(X - g(Y))^2]$. This is precisely the orthogonal projection of $X$ onto the closed\n",
    "subspace of $\\sigma(Y)$-measurable $L^2$ functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Expectation as $L^2$ Projection\n",
    "\n",
    "Among all functions $g(Y)$, the conditional expectation $E[X \\mid Y]$ is the unique\n",
    "(a.s.) minimizer of $E[(X - g(Y))^2]$.\n",
    "\n",
    "**Analogy:** Just as in Euclidean geometry, the closest point on a subspace to a given\n",
    "point is found by orthogonal projection, $E[X \\mid Y]$ is the \"closest\" $\\sigma(Y)$-measurable\n",
    "approximation to $X$ in the $L^2$ sense.\n",
    "\n",
    "We demonstrate this with a bivariate normal $(X, Y)$ where\n",
    "$E[X \\mid Y] = \\mu_X + \\rho \\frac{\\sigma_X}{\\sigma_Y}(Y - \\mu_Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 100_000\n",
    "rho = 0.7\n",
    "\n",
    "# Generate bivariate normal (X, Y) with correlation rho\n",
    "Z1 = np.random.standard_normal(n)\n",
    "Z2 = np.random.standard_normal(n)\n",
    "Y = Z1\n",
    "X = rho * Z1 + np.sqrt(1 - rho**2) * Z2  # X = rho*Y + sqrt(1-rho^2)*Z2\n",
    "\n",
    "# True conditional expectation: E[X|Y] = rho * Y\n",
    "cond_exp_true = rho * Y\n",
    "\n",
    "# Compare MSE: E[X|Y] vs other candidate functions g(Y)\n",
    "candidates = {\n",
    "    r'$g(Y) = 0$ (unconditional mean)': np.zeros(n),\n",
    "    r'$g(Y) = 0.3\\,Y$': 0.3 * Y,\n",
    "    r'$g(Y) = 0.5\\,Y$': 0.5 * Y,\n",
    "    r'$g(Y) = \\rho\\,Y$ (true $E[X|Y]$)': cond_exp_true,\n",
    "    r'$g(Y) = Y$': Y,\n",
    "    r'$g(Y) = Y^2 - 1$': Y**2 - 1,\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar chart of MSE for each candidate\n",
    "names = list(candidates.keys())\n",
    "mses = [np.mean((X - g)**2) for g in candidates.values()]\n",
    "colors = ['#aaa'] * len(names)\n",
    "colors[3] = 'steelblue'  # highlight the true CE\n",
    "ax1.barh(range(len(names)), mses, color=colors, edgecolor='navy', alpha=0.85)\n",
    "ax1.set_yticks(range(len(names)))\n",
    "ax1.set_yticklabels(names, fontsize=10)\n",
    "ax1.set_xlabel('Mean Squared Error $E[(X - g(Y))^2]$', fontsize=11)\n",
    "ax1.set_title(f'$L^2$ Projection: $E[X|Y]$ minimizes MSE (rho={rho})', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "for i, v in enumerate(mses):\n",
    "    ax1.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# Scatter plot with conditional expectation line\n",
    "idx = np.random.choice(n, 3000, replace=False)\n",
    "ax2.scatter(Y[idx], X[idx], alpha=0.15, s=5, color='gray', label='(Y, X) samples')\n",
    "y_sorted = np.sort(Y[idx])\n",
    "ax2.plot(y_sorted, rho * y_sorted, 'r-', linewidth=2.5, label=r'$E[X|Y] = \\rho Y$')\n",
    "ax2.plot(y_sorted, 0.5 * y_sorted, 'g--', linewidth=1.5, label=r'$g(Y) = 0.5Y$ (suboptimal)')\n",
    "ax2.set_xlabel('Y', fontsize=11)\n",
    "ax2.set_ylabel('X', fontsize=11)\n",
    "ax2.set_title('Conditional Expectation as Best Predictor', fontsize=12)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Minimum MSE achieved by E[X|Y]: {mses[3]:.6f}')\n",
    "print(f'Theoretical residual variance: 1 - rho^2 = {1 - rho**2:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $E[X \\mid Y]$: Discrete Case\n",
    "\n",
    "When $Y$ takes finitely many values $\\{y_1, \\ldots, y_k\\}$, the conditional expectation\n",
    "is simply the mean of $X$ within each level set:\n",
    "\n",
    "$$E[X \\mid Y = y_j] = \\frac{E[X \\cdot \\mathbf{1}_{\\{Y=y_j\\}}]}{P(Y=y_j)}$$\n",
    "\n",
    "We simulate a joint distribution on $(X, Y)$ where $Y \\in \\{1, 2, 3, 4\\}$\n",
    "and $X \\mid Y=y \\sim N(y^2, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n = 50_000\n",
    "\n",
    "# Discrete Y, continuous X|Y\n",
    "Y_vals = np.array([1, 2, 3, 4])\n",
    "Y_disc = np.random.choice(Y_vals, size=n, p=[0.1, 0.3, 0.4, 0.2])\n",
    "X_disc = np.random.normal(loc=Y_disc**2, scale=1.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Panel 1: Conditional distributions\n",
    "ax = axes[0]\n",
    "colors_disc = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "for i, y in enumerate(Y_vals):\n",
    "    mask = Y_disc == y\n",
    "    ax.hist(X_disc[mask], bins=40, density=True, alpha=0.5,\n",
    "            color=colors_disc[i], label=f'X | Y={y}')\n",
    "    emp_mean = np.mean(X_disc[mask])\n",
    "    ax.axvline(emp_mean, color=colors_disc[i], linestyle='--', linewidth=2)\n",
    "ax.set_title('Conditional Distributions of X | Y', fontsize=12)\n",
    "ax.set_xlabel('X')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: E[X|Y] as a function of Y\n",
    "ax = axes[1]\n",
    "empirical_ce = np.array([np.mean(X_disc[Y_disc == y]) for y in Y_vals])\n",
    "theoretical_ce = Y_vals**2\n",
    "ax.bar(Y_vals - 0.15, empirical_ce, width=0.3, color='steelblue',\n",
    "       edgecolor='navy', alpha=0.8, label='Empirical $E[X|Y]$')\n",
    "ax.bar(Y_vals + 0.15, theoretical_ce, width=0.3, color='coral',\n",
    "       edgecolor='darkred', alpha=0.8, label='Theoretical $Y^2$')\n",
    "ax.set_xlabel('Y', fontsize=11)\n",
    "ax.set_ylabel('$E[X | Y]$', fontsize=11)\n",
    "ax.set_title('Conditional Expectation as Function of Y', fontsize=12)\n",
    "ax.set_xticks(Y_vals)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Verification table\n",
    "ax = axes[2]\n",
    "ax.axis('off')\n",
    "p_y = np.array([0.1, 0.3, 0.4, 0.2])\n",
    "table_data = []\n",
    "for i, y in enumerate(Y_vals):\n",
    "    table_data.append([f'{y}', f'{p_y[i]:.1f}',\n",
    "                       f'{empirical_ce[i]:.3f}', f'{theoretical_ce[i]:.1f}'])\n",
    "table = ax.table(cellText=table_data,\n",
    "                 colLabels=['Y', 'P(Y)', 'Empirical E[X|Y]', 'True E[X|Y]'],\n",
    "                 cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 1.8)\n",
    "ax.set_title('Discrete Conditional Expectation', fontsize=12, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $E[X \\mid Y]$: Continuous Case (Binning Estimator)\n",
    "\n",
    "When both $X$ and $Y$ are continuous, we estimate $E[X \\mid Y = y]$ by partitioning\n",
    "the range of $Y$ into bins and averaging $X$ within each bin. As the bin width\n",
    "shrinks (and sample size grows), this converges to the true conditional expectation.\n",
    "\n",
    "We use a nonlinear relationship: $X = \\sin(2Y) + 0.5\\,\\varepsilon$ to show that\n",
    "conditional expectation captures the nonlinear signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "n = 80_000\n",
    "\n",
    "Y_cont = np.random.uniform(-np.pi, np.pi, n)\n",
    "noise = np.random.standard_normal(n)\n",
    "X_cont = np.sin(2 * Y_cont) + 0.5 * noise  # nonlinear relationship\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Panel 1: Different bin resolutions\n",
    "ax = axes[0]\n",
    "idx_plot = np.random.choice(n, 2000, replace=False)\n",
    "ax.scatter(Y_cont[idx_plot], X_cont[idx_plot], alpha=0.1, s=3, color='gray')\n",
    "\n",
    "for n_bins, color, lw in [(10, '#e74c3c', 1.5), (30, '#3498db', 2.0), (100, '#2ecc71', 2.5)]:\n",
    "    bin_edges = np.linspace(-np.pi, np.pi, n_bins + 1)\n",
    "    bin_idx = np.digitize(Y_cont, bin_edges) - 1\n",
    "    bin_idx = np.clip(bin_idx, 0, n_bins - 1)\n",
    "    bin_means = np.array([np.mean(X_cont[bin_idx == b]) if np.sum(bin_idx == b) > 0\n",
    "                          else 0 for b in range(n_bins)])\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    ax.plot(bin_centers, bin_means, color=color, linewidth=lw,\n",
    "            label=f'{n_bins} bins', alpha=0.9)\n",
    "\n",
    "y_grid = np.linspace(-np.pi, np.pi, 300)\n",
    "ax.plot(y_grid, np.sin(2 * y_grid), 'k-', linewidth=2, label='True $E[X|Y]$')\n",
    "ax.set_xlabel('Y', fontsize=11)\n",
    "ax.set_ylabel('X', fontsize=11)\n",
    "ax.set_title('Binning Estimator Converges to $E[X|Y]$', fontsize=12)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: MSE vs number of bins (bias-variance tradeoff)\n",
    "ax = axes[1]\n",
    "bin_counts = [5, 10, 20, 40, 60, 80, 100, 150, 200, 300, 500]\n",
    "mse_bins = []\n",
    "for nb in bin_counts:\n",
    "    edges = np.linspace(-np.pi, np.pi, nb + 1)\n",
    "    bidx = np.clip(np.digitize(Y_cont, edges) - 1, 0, nb - 1)\n",
    "    bmeans = np.array([np.mean(X_cont[bidx == b]) if np.sum(bidx == b) > 0\n",
    "                       else 0 for b in range(nb)])\n",
    "    predicted = bmeans[bidx]\n",
    "    mse_bins.append(np.mean((X_cont - predicted)**2))\n",
    "\n",
    "ax.semilogx(bin_counts, mse_bins, 'o-', color='steelblue', linewidth=2, markersize=6)\n",
    "ax.axhline(0.25, color='red', linestyle='--', linewidth=1.5,\n",
    "           label=r'Noise floor $\\sigma^2 = 0.25$')\n",
    "ax.set_xlabel('Number of Bins', fontsize=11)\n",
    "ax.set_ylabel('MSE', fontsize=11)\n",
    "ax.set_title('Bias-Variance Tradeoff in Binning', fontsize=12)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Conditional variance\n",
    "ax = axes[2]\n",
    "n_bins_cv = 50\n",
    "edges_cv = np.linspace(-np.pi, np.pi, n_bins_cv + 1)\n",
    "bidx_cv = np.clip(np.digitize(Y_cont, edges_cv) - 1, 0, n_bins_cv - 1)\n",
    "bin_vars = np.array([np.var(X_cont[bidx_cv == b]) if np.sum(bidx_cv == b) > 5\n",
    "                     else np.nan for b in range(n_bins_cv)])\n",
    "bin_ctrs = (edges_cv[:-1] + edges_cv[1:]) / 2\n",
    "valid = ~np.isnan(bin_vars)\n",
    "ax.bar(bin_ctrs[valid], bin_vars[valid], width=2*np.pi/n_bins_cv * 0.85,\n",
    "       color='steelblue', edgecolor='navy', alpha=0.7)\n",
    "ax.axhline(0.25, color='red', linestyle='--', linewidth=2,\n",
    "           label=r'True $\\mathrm{Var}(X|Y) = 0.25$')\n",
    "ax.set_xlabel('Y', fontsize=11)\n",
    "ax.set_ylabel('$\\mathrm{Var}(X \\mid Y)$', fontsize=11)\n",
    "ax.set_title('Conditional Variance (Homoscedastic)', fontsize=12)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tower Property: $E[E[X \\mid Y]] = E[X]$\n",
    "\n",
    "The tower property (law of iterated expectations) states that taking the\n",
    "outer expectation of a conditional expectation recovers the unconditional mean.\n",
    "More generally, if $\\mathcal{G} \\subseteq \\mathcal{H}$, then\n",
    "$E[E[X \\mid \\mathcal{H}] \\mid \\mathcal{G}] = E[X \\mid \\mathcal{G}]$.\n",
    "\n",
    "We verify this numerically across several joint distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "n = 200_000\n",
    "\n",
    "# --- Test 1: Bivariate normal ---\n",
    "rho_t = 0.6\n",
    "Z1 = np.random.standard_normal(n)\n",
    "Z2 = np.random.standard_normal(n)\n",
    "Y1 = Z1\n",
    "X1 = 3 + rho_t * Z1 + np.sqrt(1 - rho_t**2) * Z2  # E[X] = 3\n",
    "\n",
    "# Empirical E[X|Y] via binning\n",
    "n_bins_t = 100\n",
    "edges1 = np.linspace(Y1.min(), Y1.max(), n_bins_t + 1)\n",
    "bidx1 = np.clip(np.digitize(Y1, edges1) - 1, 0, n_bins_t - 1)\n",
    "ce1 = np.array([np.mean(X1[bidx1 == b]) for b in range(n_bins_t)])\n",
    "ce1_all = ce1[bidx1]  # E[X|Y] for each sample\n",
    "\n",
    "# --- Test 2: X|Y ~ Poisson(Y), Y ~ Exp(1) ---\n",
    "Y2 = np.random.exponential(1.0, n)\n",
    "X2 = np.random.poisson(lam=Y2)\n",
    "\n",
    "edges2 = np.linspace(0, np.percentile(Y2, 99), n_bins_t + 1)\n",
    "bidx2 = np.clip(np.digitize(Y2, edges2) - 1, 0, n_bins_t - 1)\n",
    "ce2 = np.array([np.mean(X2[bidx2 == b]) if np.sum(bidx2 == b) > 0\n",
    "                else 0 for b in range(n_bins_t)])\n",
    "ce2_all = ce2[bidx2]\n",
    "\n",
    "# --- Test 3: Nonlinear --- X = Y^3 + epsilon\n",
    "Y3 = np.random.uniform(-1, 1, n)\n",
    "X3 = Y3**3 + 0.2 * np.random.standard_normal(n)\n",
    "\n",
    "edges3 = np.linspace(-1, 1, n_bins_t + 1)\n",
    "bidx3 = np.clip(np.digitize(Y3, edges3) - 1, 0, n_bins_t - 1)\n",
    "ce3 = np.array([np.mean(X3[bidx3 == b]) for b in range(n_bins_t)])\n",
    "ce3_all = ce3[bidx3]\n",
    "\n",
    "# --- Display results ---\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.axis('off')\n",
    "\n",
    "tests = [\n",
    "    ('Bivariate Normal', np.mean(X1), np.mean(ce1_all), 3.0),\n",
    "    ('Poisson-Exponential', np.mean(X2), np.mean(ce2_all), 1.0),\n",
    "    ('Cubic + noise', np.mean(X3), np.mean(ce3_all), 0.0),\n",
    "]\n",
    "\n",
    "table_data = []\n",
    "for name, ex, ece, theory in tests:\n",
    "    table_data.append([name, f'{ex:.5f}', f'{ece:.5f}',\n",
    "                       f'{theory:.5f}', f'{abs(ex - ece):.6f}'])\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                 colLabels=['Distribution', 'E[X]', 'E[E[X|Y]]',\n",
    "                            'Theoretical E[X]', '|Difference|'],\n",
    "                 cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 2.0)\n",
    "\n",
    "# Color header\n",
    "for j in range(5):\n",
    "    table[0, j].set_facecolor('#d4e6f1')\n",
    "\n",
    "ax.set_title('Tower Property Verification: $E[E[X|Y]] = E[X]$',\n",
    "             fontsize=14, pad=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Tower property verified: E[E[X|Y]] matches E[X] up to sampling error.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Expectation as Smoothing\n",
    "\n",
    "Conditional expectation acts as a **smoothing** operator. Given the joint\n",
    "scatter $(Y, X)$, the conditional expectation $E[X \\mid Y]$ replaces the noisy\n",
    "cloud with a smooth curve. We demonstrate how finer $\\sigma$-algebras\n",
    "preserve more detail while coarser ones smooth more aggressively.\n",
    "\n",
    "We also show the **total variance decomposition:**\n",
    "$$\\mathrm{Var}(X) = E[\\mathrm{Var}(X \\mid Y)] + \\mathrm{Var}(E[X \\mid Y])$$\n",
    "\n",
    "which says the total variance of $X$ is split into the average within-group\n",
    "variance and the between-group variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)\n",
    "n = 100_000\n",
    "\n",
    "# Generate data with heteroscedastic noise\n",
    "Y_sm = np.random.uniform(0, 4 * np.pi, n)\n",
    "noise_scale = 0.3 + 0.5 * np.abs(np.sin(Y_sm / 2))\n",
    "X_sm = np.cos(Y_sm) + 0.3 * Y_sm / (4 * np.pi) + noise_scale * np.random.standard_normal(n)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Panel 1: Raw scatter\n",
    "ax = axes[0, 0]\n",
    "idx_sm = np.random.choice(n, 3000, replace=False)\n",
    "ax.scatter(Y_sm[idx_sm], X_sm[idx_sm], alpha=0.15, s=3, color='gray')\n",
    "ax.set_title('Raw Joint Distribution $(Y, X)$', fontsize=12)\n",
    "ax.set_xlabel('Y'); ax.set_ylabel('X')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Smoothing at different resolutions\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(Y_sm[idx_sm], X_sm[idx_sm], alpha=0.08, s=2, color='gray')\n",
    "resolutions = [(5, '#e74c3c', 'Very coarse (5 bins)'),\n",
    "               (20, '#3498db', 'Medium (20 bins)'),\n",
    "               (100, '#2ecc71', 'Fine (100 bins)')]\n",
    "for nb, color, label in resolutions:\n",
    "    edges = np.linspace(0, 4 * np.pi, nb + 1)\n",
    "    bidx = np.clip(np.digitize(Y_sm, edges) - 1, 0, nb - 1)\n",
    "    bmeans = np.array([np.mean(X_sm[bidx == b]) if np.sum(bidx == b) > 0\n",
    "                       else np.nan for b in range(nb)])\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "    valid = ~np.isnan(bmeans)\n",
    "    ax.plot(centers[valid], bmeans[valid], color=color, linewidth=2, label=label)\n",
    "\n",
    "ax.set_title('$E[X|Y]$: Coarser $\\\\sigma$-algebra = More Smoothing', fontsize=12)\n",
    "ax.set_xlabel('Y'); ax.set_ylabel('$E[X|Y]$')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Total variance decomposition\n",
    "ax = axes[1, 0]\n",
    "bin_list = np.arange(2, 202, 5)\n",
    "var_ce = []  # Var(E[X|Y])\n",
    "e_cv = []    # E[Var(X|Y)]\n",
    "\n",
    "for nb in bin_list:\n",
    "    edges = np.linspace(0, 4 * np.pi, nb + 1)\n",
    "    bidx = np.clip(np.digitize(Y_sm, edges) - 1, 0, nb - 1)\n",
    "    bmeans = np.array([np.mean(X_sm[bidx == b]) if np.sum(bidx == b) > 5\n",
    "                       else np.nan for b in range(nb)])\n",
    "    bvars = np.array([np.var(X_sm[bidx == b]) if np.sum(bidx == b) > 5\n",
    "                      else np.nan for b in range(nb)])\n",
    "    bcounts = np.array([np.sum(bidx == b) for b in range(nb)])\n",
    "    valid = ~np.isnan(bmeans)\n",
    "    # Var(E[X|Y]) and E[Var(X|Y)]\n",
    "    ce_values = bmeans[valid]\n",
    "    cv_values = bvars[valid]\n",
    "    weights = bcounts[valid] / bcounts[valid].sum()\n",
    "    var_ce.append(np.sum(weights * (ce_values - np.sum(weights * ce_values))**2))\n",
    "    e_cv.append(np.sum(weights * cv_values))\n",
    "\n",
    "total_var = np.var(X_sm)\n",
    "ax.plot(bin_list, var_ce, 'r-', linewidth=2, label='$\\mathrm{Var}(E[X|Y])$ (explained)')\n",
    "ax.plot(bin_list, e_cv, 'b-', linewidth=2, label='$E[\\mathrm{Var}(X|Y)]$ (residual)')\n",
    "ax.plot(bin_list, np.array(var_ce) + np.array(e_cv), 'k--', linewidth=1.5,\n",
    "        label='Sum (should = Var(X))')\n",
    "ax.axhline(total_var, color='gray', linestyle=':', linewidth=1.5,\n",
    "           label=f'Var(X) = {total_var:.4f}')\n",
    "ax.set_xlabel('Number of Bins (resolution of $\\\\sigma(Y)$)', fontsize=11)\n",
    "ax.set_ylabel('Variance', fontsize=11)\n",
    "ax.set_title('Total Variance Decomposition', fontsize=12)\n",
    "ax.legend(fontsize=8, loc='center right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 4: Orthogonality of residual\n",
    "ax = axes[1, 1]\n",
    "n_bins_orth = 60\n",
    "edges_o = np.linspace(0, 4 * np.pi, n_bins_orth + 1)\n",
    "bidx_o = np.clip(np.digitize(Y_sm, edges_o) - 1, 0, n_bins_orth - 1)\n",
    "bmeans_o = np.array([np.mean(X_sm[bidx_o == b]) for b in range(n_bins_orth)])\n",
    "ce_all = bmeans_o[bidx_o]\n",
    "residual = X_sm - ce_all\n",
    "\n",
    "# Compute correlation between residual and several sigma(Y)-measurable functions\n",
    "test_funcs = {\n",
    "    '$Y$': Y_sm,\n",
    "    '$Y^2$': Y_sm**2,\n",
    "    '$\\\\sin(Y)$': np.sin(Y_sm),\n",
    "    '$E[X|Y]$': ce_all,\n",
    "    '$\\\\cos(3Y)$': np.cos(3 * Y_sm),\n",
    "}\n",
    "\n",
    "func_names = list(test_funcs.keys())\n",
    "correlations = [np.corrcoef(residual, f)[0, 1] for f in test_funcs.values()]\n",
    "\n",
    "bars = ax.bar(range(len(func_names)), correlations, color='steelblue',\n",
    "              edgecolor='navy', alpha=0.8)\n",
    "ax.set_xticks(range(len(func_names)))\n",
    "ax.set_xticklabels(func_names, fontsize=10)\n",
    "ax.set_ylabel('Correlation with Residual', fontsize=11)\n",
    "ax.set_title('Orthogonality: $X - E[X|Y] \\\\perp \\\\sigma(Y)$-measurable functions', fontsize=11)\n",
    "ax.axhline(0, color='red', linewidth=1.5)\n",
    "ax.set_ylim(-0.1, 0.1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "for i, v in enumerate(correlations):\n",
    "    ax.text(i, v + 0.005 * np.sign(v), f'{v:.4f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Total Var(X) = {total_var:.5f}')\n",
    "print(f'Var(E[X|Y]) + E[Var(X|Y)] = {var_ce[-1]:.5f} + {e_cv[-1]:.5f} = {var_ce[-1]+e_cv[-1]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **$L^2$ projection:** $E[X \\mid Y]$ is the unique $\\sigma(Y)$-measurable function minimizing $E[(X - g(Y))^2]$. No other function of $Y$ can predict $X$ more accurately in the mean-squared sense.\n",
    "\n",
    "2. **Discrete case:** Computing $E[X \\mid Y]$ reduces to averaging $X$ over each level set $\\{Y = y\\}$. The result is a function $g(y) = E[X \\mid Y = y]$.\n",
    "\n",
    "3. **Continuous case (binning):** For continuous $Y$, we partition its range into bins and average $X$ within each. Finer bins reduce bias but increase variance -- the classic bias-variance tradeoff.\n",
    "\n",
    "4. **Tower property:** $E[E[X \\mid Y]] = E[X]$ always holds. This is a direct consequence of the projection interpretation: projecting twice onto a larger then smaller subspace gives the same result as projecting onto the smaller one directly.\n",
    "\n",
    "5. **Smoothing interpretation:** Coarser $\\sigma$-algebras produce more smoothing (averaging over larger groups), while finer ones preserve more detail. In the limit, $\\sigma(Y) = \\sigma(X, Y)$ gives $E[X \\mid X, Y] = X$ (no smoothing).\n",
    "\n",
    "6. **Orthogonality:** The residual $X - E[X \\mid Y]$ is orthogonal to all $\\sigma(Y)$-measurable functions, which drives the total variance decomposition $\\mathrm{Var}(X) = \\mathrm{Var}(E[X \\mid Y]) + E[\\mathrm{Var}(X \\mid Y)]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}